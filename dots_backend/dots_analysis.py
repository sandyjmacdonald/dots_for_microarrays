#!/usr/bin/env python3

import warnings
import pandas as pd
import numpy as np
import scipy.cluster.hierarchy as hac

from sklearn.decomposition import PCA
from itertools import combinations
from scipy.stats import ttest_ind, f_oneway
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.multicomp import MultiComparison
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.cluster import KMeans

from .dots_arrays import Experiment


def run_pca(experiment):
    '''Run PCA when given an experiment instance or data frame with expression values.

    Args:
        experiment (Experiment instance): An instance of the Experiment class.

    Returns:
        A Pandas data frame with results of PCA analysis.

    '''

    #  The below if/elif checks whether experiment passed to function is an instance of the
    #  Experiment class or just a data frame with expression values in.
    if isinstance(experiment, Experiment):
        df = experiment.get_exp_values().T
    elif isinstance(experiment, pd.DataFrame):
        df = experiment.T

    #  Run the PCA, get the scores and unzip tuples into separate lists of x and y values.
    pca = PCA(n_components=3)
    pca_fit = pca.fit_transform(df)
    vals = [(x[0], x[1]) for x in pca_fit]
    xvals, yvals = list(zip(*vals))

    #  Convert the data into a dictionary for easy conversion into a Pandas data frame.
    pca_dict = {'xvals': xvals, 'yvals': yvals, 'sampleid': list(df.index), 'group': [x.split('_')[0] for x in list(df.index)]}
    pca_df = pd.DataFrame(pca_dict)

    return pca_df

def get_fold_changes(experiment):
    '''Calculate pairwise fold change and log fold change values.

    Args:
        experiment (Experiment instance): An instance of the Experiment class.

    Returns:
        A new Pandas data frame with pairwise fold change and log fold change values.

    '''

    groups = experiment.get_groups()
    pairs = list(map(list, list(combinations(groups, 2))))

    if all([g.isdigit() for g in groups]):
        pairs = sorted(pairs, key=lambda x:x[0])

    samples = experiment.get_sampleids()
    df = experiment.df
    new_df = df.iloc[:, :5].copy()

    for group in groups:
        ids = [sample for sample in samples if group == sample.split('_')[0]]
        new_df['mean_' + group] = df[ids].mean(axis=1)

    del df

    # For each pair, calculate mean values for each group, fold changes and log2 fold changes.
    for pair in pairs:
        if all([g.isdigit() for g in pair]):
            pair.sort(key=int, reverse=True)
        else:
            pair.sort()

        name_1, name_2 = pair
        new_df['abs_mean_diff_' + name_1 + '_' + name_2] = abs((2 ** new_df['mean_' + name_1]) - (2 ** new_df['mean_' + name_2]))
        new_df['logFC_' + name_1 + '_' + name_2] = new_df['mean_' + name_1] - new_df['mean_' + name_2]
        new_df['FC_' + name_1 + '_' + name_2] = 2 ** new_df['logFC_' + name_1 + '_' + name_2]

    return new_df

def run_stats(experiment):
    '''Run independent T-test or one-way ANOVA dependent on number of groups.

    Args:
        experiment (Experiment instance): An instance of the Experiment class.

    Returns:
        A new Pandas data frame with p values, adjusted p values and Tukey HSD
        post-hoc results if there are > 2 groups.

    '''

    groups = experiment.get_groups()
    samples = experiment.get_sampleids()
    df = experiment.df
    all_vals = []

    #  Get values for each group, ready for T-test or ANOVA.
    for group in groups:
        ids = [sample for sample in samples if group == sample.split('_')[0]]
        vals = list(map(list, df[ids].values))
        all_vals.append(vals)

    #  Decide whether to use T-test or ANOVA dependent on number of groups.
    if len(groups) == 2:
        p_vals = [ttest_ind(all_vals[0][i], all_vals[1][i])[1] for i in range(len(all_vals[0]))]
    else:
        p_vals = []
        for i in range(len(all_vals[0])):
            row_vals = [all_vals[j][i] for j in range(len(groups))]
            p_val = f_oneway(*row_vals)[1]
            p_vals.append(p_val)

    #  Adjust the p values and create a new data frame with them in.
    p_val_adj = list(multipletests(p_vals, method='fdr_bh')[1])
    new_df = df.iloc[:, :5].copy()
    new_df['p_val'] = pd.Series(p_vals, index=new_df.index)
    new_df['p_val_adj'] = pd.Series(p_val_adj, index=new_df.index)

    #  Post-hoc test.

    #  Only do the post-hoc test if there are more than 2 groups, duh!
    if len(groups) > 2:
        vals_df = df[samples]
        group_ids = [sample.split('_')[0] for sample in vals_df.columns.values]
        posthoc_results = {}

        #  Run the post-hoc test on each row.
        for row in range(len(vals_df)):
            row_vals = vals_df.iloc[row]
            mc = MultiComparison(row_vals, group_ids)
            mc_groups = mc.groupsunique
            results = mc.tukeyhsd()
            significant = results.reject
            pairs = list(zip(*[x.tolist() for x in mc.pairindices]))

            #  Go through each pair and add results to the posthoc_results dictionary.
            for i in range(len(pairs)):
                pair = list(pairs[i])
                pair.sort()
                pair_name = str(mc_groups[pair[0]]) + '_' + str(mc_groups[pair[1]])
 
                if pair_name in posthoc_results:
                    posthoc_results[pair_name].append(significant[i])
                else:
                    posthoc_results[pair_name] = [significant[i]]

        #  Add the post-hoc results to the data frame.
        for pair_name in posthoc_results:
            new_df['significant_' + pair_name] = posthoc_results[pair_name]

    return new_df

def find_clusters(df, k_vals=[4, 9, 16, 25], how='hierarchical'):
    '''Find clusters, and if method is k-means run silhouette analysis
    to determine the value of k.

    Args:
        df (data frame): A data frame with normalised expression data.
        k_vals (list or range): The range over which to test k.
        how ('hierarchical' or 'kmeans'): Clustering method.

    Returns:
        A list of cluster numbers.

    '''

    #  Don't run the silhouette analysis for hierarchical clustering,
    #  just calculate the clusters using estimate of k.
    if how == 'hierarchical':
        k = int(np.sqrt((len(df) / 2.0)))
        hc = hac.linkage(df, method='average')
        optimal_clusters = hac.fcluster(hc, t=k, criterion='maxclust')

    #  If method is k-means, run silhouette analysis.
    elif how == 'kmeans':
        best_combined_score = 0
        optimal_k = 2

        #  Try values of k from range and keep track of optimal k according
        #  to silhouette score.
        for k in k_vals:
            km = KMeans(n_clusters=k, random_state=10)
            clusters = km.fit_predict(df)
            silhouette_avg = silhouette_score(df, clusters)
            sample_silhouette_values = silhouette_samples(df, clusters)
            above_mean = 0
            silhouette_sizes = []

            for i in range(k):
                ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]
                size_cluster_i = ith_cluster_silhouette_values.shape[0]
                silhouette_sizes.append(size_cluster_i)

                if max(ith_cluster_silhouette_values) > silhouette_avg:
                    above_mean += 1

            #  This combined score should pick the best value of k
            above_mean_score = float(above_mean) / k
            std_score = 1.0/np.std(silhouette_sizes) if np.std(silhouette_sizes) > 1.0 else 1.0
            combined_score = (silhouette_avg + above_mean_score + std_score) / 3

            #  Put the clusters in the new column in the data frame.
            if combined_score > best_combined_score:
                best_combined_score = combined_score
                optimal_k = k
                optimal_clusters = clusters

        optimal_clusters = [cluster + 1 for cluster in optimal_clusters]

    return optimal_clusters

def get_clusters(experiment, how='hierarchical'):
    '''Clusters significantly differentially expressed genes by expression pattern
    across the samples using hierarchical or k-means clustering and silhouette analysis
    to pick the value of k (via the find_clusters function).

    Args:
        experiment (Experiment instance): An instance of the Experiment class.
        how ('hierarchical' or 'kmeans'): Clustering method.

    Returns:
        A new Pandas data frame with fold changes, p values and clusters.

    '''

    #  Run the stats to filter genes down to significant ones only.
    stats = run_stats(experiment)
    stats = stats[['FeatureNum', 'p_val', 'p_val_adj']].copy()

    #  Get the fold changes
    fcs = get_fold_changes(experiment)
    keep_cols = [x for x in fcs.columns.values if 'logFC' in x or 'abs_mean_diff' in x]
    fc_cols = [x for x in fcs.columns.values if 'logFC' in x]
    fcs = fcs[['FeatureNum'] + keep_cols].copy()

    norm_exp_cols = experiment.get_sampleids()

    abs_mean_diff_cols = [x for x in fcs.columns.values if 'abs_mean_diff' in x]

    #  Merge together the stats and fold changes data frames.
    merged_df = pd.merge(experiment.df, stats, on='FeatureNum')
    merged_df = pd.merge(merged_df, fcs, on='FeatureNum')

    #  Filter the merged data frame to leave only significantly differentially
    #  expressed genes (adj. p < 0.05. Also, increase the fold change cutoff until there
    #  are less than 2,500 rows left in the data frame (so that heat maps can be drawn).
    filtered_df = merged_df[(merged_df['p_val_adj'] < 0.05) & ((abs(merged_df[fc_cols]) > np.log2(float(1))).any(1) == True) & ((merged_df[abs_mean_diff_cols] > 0.5).any(1) == True)].copy()
    i = 2

    while len(filtered_df) * len(experiment.get_sampleids()) > 40000:
        filtered_df = merged_df[(merged_df['p_val_adj'] < 0.05) & ((abs(merged_df[fc_cols]) > np.log2(float(i))).any(1) == True) & ((merged_df[abs_mean_diff_cols] > 0.5).any(1) == True)].copy()
        i += 1

    #  Clean up.
    del merged_df
    del stats
    del fcs

    #  A good guesstimate for k.
    k_limit = int(np.sqrt((len(filtered_df) / 2)))

    #  Catches numpy warnings about means of empty slices.
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=RuntimeWarning)

        #  Hierarchical clustering.
        if how == 'hierarchical':
            clusters = find_clusters(filtered_df[norm_exp_cols], how='hierarchical')
            filtered_df['cluster'] = clusters

        #  K-means clustering with silhouette analysis to determine value of k.
        elif how == 'kmeans':
            clusters = find_clusters(filtered_df[norm_exp_cols], k_vals=list(range(3, k_limit)), how='kmeans')
            filtered_df['cluster'] = clusters

    #  Sort the data frame by cluster and mean expression across samples.
    filtered_df['mean_norm_expression'] = filtered_df[norm_exp_cols].mean(axis=0)
    filtered_df.sort_values(by=['cluster', 'mean_norm_expression'], ascending=[True, False], inplace=True)
    filtered_df = filtered_df.reset_index(drop=True)

    return filtered_df

def write_fcs_stats(experiment, outfile='foldchanges_stats.txt'):
    '''Creates a tab-separated table with a full list of fold changes,
    p values, adjusted p values and post hoc results.

    Args:
        experiment (Experiment instance): An instance of the Experiment class.
        outfile (string): The name of the table-separated table to be created.

    '''

    #  Run the stats and fold changes and merge them into a single data frame.
    stats = run_stats(experiment)
    posthoc_cols = [colname for colname in stats.columns.values if 'significant' in colname]
    stats = stats[['FeatureNum', 'p_val', 'p_val_adj'] + posthoc_cols]
    fcs = get_fold_changes(experiment)
    fc_cols = [colname for colname in fcs.columns.values if not 'abs_mean_diff_' in colname]
    merged_df = pd.merge(fcs, stats, on='FeatureNum')

    #  Define the order of the columns in the data frame.
    colnames = list(merged_df.columns.values)
    global col_order
    col_order = ['mean', 'FC', 'logFC', 'abs_mean_diff', 'p_val', 'adj_p_val', 'significant']

    #  Function to custom sort the columns.
    def keyfunc(col):
        for c in col_order:
            if col.startswith(c):
                return (col_order.index(c), col.lstrip(c + '_'))
                break

    #  Sort the columns.
    sorted_colnames = colnames[:5] + sorted(colnames[5:], key=keyfunc)
    merged_df = merged_df[sorted_colnames]

    #  Fix the type of the FeatureNum column and sort it.
    merged_df['FeatureNum'] = merged_df['FeatureNum'].astype(int)
    merged_df.sort_values(by='FeatureNum', ascending=True, inplace=True)

    #  Write the table.
    merged_df.to_csv(outfile, sep='\t', index=False)

def write_normalised_expression(experiment, outfile='normalised_expression.txt'):
    '''Creates a tab-separated table with all of the normalised expression values.

    Args:
        experiment (Experiment instance): An instance of the Experiment class.
        outfile (string): The name of the table-separated table to be created.

    '''

    #  Read in the experiment.
    experiment_df = experiment.df

    #  Sort the values columns.
    colnames = list(experiment_df.columns.values)
    sorted_colnames = colnames[:5] + sorted(colnames[5:])
    experiment_df = experiment_df[sorted_colnames]

    #  Write the table.
    experiment_df.to_csv(outfile, sep='\t', index=False)
